# chatbot.py

import json
import logging
import inspect
from typing import Any, Callable, List, Dict
from ollama import chat, ChatResponse
from tools import SCRAPING_TOOLS
from tool_executor import ToolExecutor

logging.basicConfig(
    level=logging.INFO,
    format="[%(asctime)s] %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

# Tool names that require an output directory
TOOLS_NEED_OUTPUT_DIR = {"download_file", "download_files_by_type", "scrape", "focused_scrape_files_by_terms"}

class ChatBot:
    def __init__(self, model: str, tools: List[Callable[..., Any]]):
        self.model = model
        self.executor = ToolExecutor(tools)
        self.messages: List[Dict[str, Any]] = []

    def send(self, user_input: str, progress_callback=None, output_dir='downloads') -> str | None:
        logging.info(f"[RECEIVED USER] {user_input}")
        self.messages.append({"role": "user", "content": user_input})

        try:
            response: ChatResponse = chat(
                model=self.model, messages=self.messages, tools=list(self.executor.tools.values())
            )

            if response.message.tool_calls:
                call = response.message.tool_calls[0]
                fn_name = call.function.name
                args = dict(call.function.arguments)

                if progress_callback:
                    param_str = ', '.join(f"{k}={v!r}" for k, v in args.items())
                    progress_callback(f"Calling tool: `{fn_name}({param_str})`")
                    logging.info(f"[TOOL CALL] {fn_name}({param_str})")

                if 'output_dir' in self.executor.get_signature(fn_name):
                    if 'output_dir' not in args or args['output_dir'] == 'downloads':
                        args['output_dir'] = output_dir

                result = self.executor.execute(fn_name, args)

                self.messages.append(response.message)  # assistant's tool call
                self.messages.append({
                    "role": "tool",
                    "name": fn_name,
                    "content": json.dumps(result)
                })

                response = chat(model=self.model, messages=self.messages, tools=list(self.executor.tools.values()))

            reply = response.message.content
            self.messages.append({"role": "assistant", "content": reply})
            logging.info(f"[BOT REPLY] {reply}")
            return reply

        except Exception as e:
            logging.exception(f"[ERROR] Exception in ChatBot.send: {e}")
            raise
if __name__ == "__main__":
    bot = ChatBot(model="webscraper", tools=SCRAPING_TOOLS)
    print("Chatbot ready! Type your questions (or 'quit' to exit).")
    while True:
        user_in = input("You: ")
        if user_in.strip().lower() == "quit":
            break
        bot_reply = bot.send(user_in)
        print("Bot:", bot_reply)
# hello.py  

def add(a: int , b: int ) -> int:
    """REturn the sum of two integers"""
    return a + b 
 # scraper.py

import requests 
from bs4 import BeautifulSoup 
from bs4.element import Comment 
from typing import List, dataclass_transform
from urllib.parse import urljoin 
from urllib.parse import urlparse
from typing import List, Dict, Any, Callable, Optional
import os 
import json 
from dataclasses import dataclass
import csv


SEARCH_TERMS = ['price','cost','patients','transparecny','estimates']
FILE_EXTENSIONS = ['.pdf','.xlsx','csv']

@dataclass 
class PageSnapshot:
    url: str
    title: str 
    headings: Dict[str,List[str]]
    main_text_snippet: str 
    json_ld : List[Dict[str,Any]]
    links: List[Dict[str,str]]
    
    def to_dict(self)->Dict[str,Any]:
        return{
                "url": self.url,
                "title": self.title,
                "headings": self.headings ,
                "main_text_snippet": self.main_text_snippet,
                "json_ld": self.json_ld,
                "links": self.links
                }

def is_visible(element):
    if element.parent.name in ['style','script','head','meat','[document]']:
        return False 
    if isinstance(element,Comment):
        return False 
    return True 


class WebScraper:
    '''
    Class based webscraper for modular maintainable workflows. 
    Encapsulated configuration ,state, and sraping logic 

    '''

    def __init__(self,
                 search_terms: Optional[List[str]],
                 file_types: Optional[List[str]],
                 max_depth: Optional[int],
                 output_dir : Optional[str],
                 urls : Optional[List[str]],
                 ):
        self.search_terms = search_terms if search_terms is not None else SEARCH_TERMS 
        self.file_types = file_types if file_types is not None else FILE_EXTENSIONS
        self.max_depth = max_depth
        self.output_dir = output_dir
        self.visited_sites = set()
        self.files_downloaded = []
        self.found_files = []
        self.urls = urls 

        os.makedirs(self.output_dir,exist_ok=True)
    
    def __repr__(self):
        return f"<Webscraper urls={len(self.urls)} terms ={self.search_terms}"


    @staticmethod
    def is_valid_http_url(url: str) ->bool:
        parsed = urlparse(url)
        return parsed.scheme in ('http','https') and bool(parsed.netloc)


    def fetch_links_from_url (self) -> List[str]:
        """
        Fetch all hyperlinks from the given URL. 
        Args:
            url(str): The url to  scrape 
        Returns:
            List[str]: A list of hyperlink URLs found on the page 
        """
        links = [] 

        for url in self.urls:
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text, "html.parser")
            
                for a_tag in soup.find_all("a",href=True):
                       full_url = urljoin(url,a_tag["href"])
                       links.append(full_url)
               
            except Exception as e: 
                print(f"Failed to fetch from {e}")
        return links
    

    def get_image_urls(self) ->List[str]:
        """
        Fetch all image hyperlinks from the given url 
        Args:
            url(str): The urls to  scrape 
        Returns:
            List[str]: A list of urls for the images found on the page
        """
        images = []


        for url in self.urls:
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text,"html.parser")

                for img_tag in soup.find_all("img",src=True):
                    full_url = urljoin(url,img_tag["src"])
                    images.append(full_url)

            except Exception as e:
                print(f"Failed to get images form {url}")

        return images


    def get_meta_data(self) -> List[Dict[str, str]]:
        results = []

        for url in self.urls:
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text, "html.parser")

                title = soup.title.string.strip() if soup.title else ""
                meta = soup.find('meta', attrs={'name': 'description'})
                description = meta['content'].strip() if meta and 'content' in meta.attrs else ""

                results.append({
                    'url': url,
                    'title': title,
                    'description': description
                })
            except Exception as e:
                print(f"Failed to extract metadata from {url}: {e}")

        return results
    

    def filter_links_by_file_type(self,links: List[str]) -> List[str]:
        return [link for link in links if any(link.lower().endswith(ex)for ex in self.file_types)]

    

    def create_output_directory(self, subfolder: Optional[str] = None) -> str: 
        """
        Ensure the base output directory (or subfolder inside it) exists 

        Args:
            subfolder(Optional[str]): An Optional subdirectory to create  within the output_folder

        Returns:
            str: Full path to created directory.

        """
        
        path = os.path.join(self.output_dir, subfolder) if subfolder else self.output_dir
        os.makedirs(path,exist_ok=True)
        return path 



    def download_files(self, links: List[str]) -> List[str]:
        downloaded = []

        for link in links:
            try:
                filename = os.path.basename(urlparse(link).path)
                
                if not filename :
                    continue 
        
                domain = urlparse(link).netloc
                ext = os.path.splitext(filename)[1].lstrip(".") or "unknown"
                save_dir = self.create_output_directory(f"{domain}/{ext}")
                

                response = requests.get(link)
                response.raise_for_status()


                filepath = os.path.join(save_dir,filename)


                with open(filepath,'wb' ) as f:
                    f.write(response.content)
                
                downloaded.append(filepath)
            except Exception as e:
                print(f"Failed to download {link}: {e}")
    
        self.files_downloaded.extend(downloaded)
        print(f"Downloaded {len(downloaded)} files.")
        return downloaded
    
    def get_heading(self) -> Dict[str, List[str]]:
        result = {f"h{i}": [] for i in range(1, 7)}
    
        for url in self.urls:
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text, "html.parser")
    
                for i in range(1, 7):
                    tags = soup.find_all(f"h{i}")
                    result[f"h{i}"].extend([tag.get_text(strip=True) for tag in tags])
    
            except Exception as e:
                print(f"Failed to extract headings from {url}: {e}")
    
        return result
    
    def search_text_for_keywords(self) -> Dict[str,List[str]]:
        """
        Search each page for configured keywords (self.search_terms)

        Returns:
            Dict[str, List[str]]: a dictionary mapping each URL to a list of matched keywords
        """
        keyword_hits = {}

        for url in self.urls:

            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text,'html.parser')
                text = soup.get_text(separator=' ').lower()

                matches = [keyword for keyword in self.search_terms if keyword.lower() in text] 

                keyword_hits[url] = matches 
    
            except Exception as e:
                print(f"Failed to search {url}: {e}")
                keyword_hits[url]= []
        return keyword_hits

    def extract_links_with_text(self) -> Dict[str,List[Dict[str,str]]]:
        """
        Extracts all links along with their anchor text. 

        Returns:
            Dict[str,List[Dict[str,str]]]: Url -> [{text:...,href: ...}]

        """

        results = {}


        for url in self.urls:
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text,"html.parser")

                link_data = []

                for a_tag in soup.find_all("a",href=True):
                    link_data.append({
                        "text": a_tag.get_text(strip=True),
                        "href": urljoin(url,a_tag["href"])
                        })
    
                results[url] = link_data 


            except Exception as e:
                print(f"Failed to extract links with text from {url}: {e}") 
                results[url] = []
        return results
  

    def extract_tables(self)-> Dict[str,List[List[List[str]]]]:
        """
        Extract HTML tables from HTML 

        Returns:
            Dictp[str,List[Listp[List[str]]]] , where each table is a list of rows and each row is a list of strings 
        """

        table_data = {}

        for url in self.urls:
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text,"html.parser")
                tables = soup.find_all("table")

                url_tables = []

                for table in tables: 
                    rows =  []
                    for row in table.find_all("tr"):
                        cells = row.find_all(["th","td"])
                        cell_text = [cell.get_text(strip=True) for cell in cells]
                        if cell_text:
                            rows.append(cell_text)

                    if rows:
                        url_tables.append(rows)

                table_data[url] = url_tables

            except Exception as e:
                print(f"Failed to extract tables from {url}")
                table_data[url] = []

            return table_data



    def extract_main_text(self)->Dict[str, str]:
        """
        Extracts the main text from a URL

        Returns:
            
        """


        
        result = {}

        for url in self.urls :
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text, "html.parser")

                texts = soup.find_all(string=True)
                visible_texts= filter(is_visible, texts)
                joined = " ".join(t.strip() for t in visible_texts if t.strip())

                result[url] = joined 

            except Exception as e:
                print(f"Failed to extract main text from {url}: {e}")
                result[url] = []


        return result 


    def extract_json_ld(self)->Dict[str,List[Dict]]:

        """
        Extract JSON-LD structured data from each page


        Returns:
            Dict[str,List[Dict]]: Mapping of URL -> List of JSON-LD object

        """
        
        json_data = {}

        for url in self.urls:
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text,"html.parser")

                scripts = soup.find_all("script", type="application/json")

                data  = []

                for script in scripts:
                    try:
                        parsed = json.loads(script.string)

                        if isinstance(parsed,list):
                            data.extend(parsed)
                        else:
                            data.append(parsed)


                    except Exception as inner_e:
                        print(f"Error parsing JSON-LD on {url}:{inner_e}")
            except Exception as e:
                print(f"Failed to extract JSON-LD from {url}:{e}")
                json_data[url]= []


        return json_data


    def get_structured_page_snapshot(self)->List[PageSnapshot]:
        """
        Returns a structured summary of each URL's content.
        Includes title, headings, main text snippet, JSON-LD, and links with text.

        Returns:
            List[Dict]: one dict per URL 
        """
        

        snapshots = [] 

        for url in self.urls:
            try:
                response = requests.get(url)
                soup = BeautifulSoup(response.text,"html.parser")

                # Title
                title = soup.title.string.strip() if soup.title else"" 

                # Headings 
                headings = {
                        f"h{i}":[tag.get_text(strip=True) for tag in soup.find_all(f"h{i}")]
                        for i in range(1,7)
                        }
                
                # JSON-LD
                texts = soup.find_all(string=True)
                visible_texts = filter(is_visible,texts)
                joined_text = " ".join(t.strip() for t in visible_texts if t.strip())
                snippet = joined_text[:1000] + "..." if len(joined_text)>1000 else joined_text

                json_ld = []

                for script in soup.find_all("script",type="application/ld+json"):
                    try:
                        parsed = json.loads(script.string)
                        json_ld.extend(parsed if isinstance(parsed,list) else [parsed])


                    except Exception as e:
                        continue 
    
                
                links = [
                        {"text": a.get_text(strip=True),"href": urljoin(url,a["href"])}
                        for a in soup.find_all("a",href=True)
                        ]

                snapshots.append(PageSnapshot(
                    url = url,
                    title = title,
                    headings = headings, 
                    main_text_snippet = snippet,
                    json_ld= json_ld,
                    links = links

                    ))
            except Exception as e:
                print(f"Snapshot Failed for {url}:{e}")
                snapshots.append(PageSnapshot(
                    url = url ,
                    title = "",
                    headings={f"h{i}": [] for i in range(1,7)},
                    main_text_snippet="",
                    json_ld=[],
                    links=[]
                    ))
        return snapshots


    def export_snapshots_to_json(self,filepath: str, snapshots: List[PageSnapshot])-> None:
        """
        Serializes a list of PageSnapshot obeject to A JSON file .


        Args:
            filepath(str) :Path to output JSON file 
            snapshots (List[PageSnapshot]): Snapshots to write
        """

        data = [snap.to_dict() for snap in snapshots]
        with open(filepath,"w",encoding="utf-8") as f:
            json.dump(data,f,ensure_ascii=False, indent=2)
    
    def load_snapshots_from_json(self, path: str) -> List[PageSnapshot]:
        with open(path,'r',encoding="utf-8") as f:
            data = json.load(f)
        return [PageSnapshot(**item) for item in data]


    def export_snapshots_to_csv(self, filepath: str, snapshots: List[PageSnapshot]) -> None:
        """
        Exports snapshots to a flat CSV file (summary view).
        Each row includes basic fields plus heading and link counts.
    
        Args:
            filepath (str): Path to CSV file to write.
            snapshots (List[PageSnapshot]): Snapshots to serialize.
        """
        with open(filepath, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=[
                "url", "title", "main_text_snippet", 
                "num_links", "num_headings", "num_json_ld"
            ])
            writer.writeheader()
            for snap in snapshots:
                writer.writerow({
                    "url": snap.url,
                    "title": snap.title,
                    "main_text_snippet": snap.main_text_snippet[:300],  # shorten snippet
                    "num_links": len(snap.links),
                    "num_headings": sum(len(hs) for hs in snap.headings.values()),
                    "num_json_ld": len(snap.json_ld)
                })
    
    
    def load_snapshots_from_csv(self, path: str) -> List[Dict[str, Any]]:
        """
        Loads summary snapshot data from a CSV file.
    
        Returns:
            List[Dict[str, Any]]
        """
        with open(path, mode='r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            return [row for row in reader]
# tools.py

from typing import List, Dict, Any
from pathlib import Path
from python_llm_toolkit.scraper import WebScraper, SEARCH_TERMS, FILE_EXTENSIONS

DEFAULT_OUTPUT_DIR = str(Path("downloads").resolve())


def get_page_metadata(urls: List[str]) -> List[Dict[str, str]]:
    """
    Extract metadata (title and description) from a list of web pages.

    Args:
        urls (List[str]): List of target URLs.

    Returns:
        List[Dict[str, str]]: List of metadata dictionaries for each page.
    """
    scraper = WebScraper(
        urls=urls,
        output_dir=DEFAULT_OUTPUT_DIR,
        file_types=FILE_EXTENSIONS,
        search_terms=SEARCH_TERMS,
        max_depth=5
    )
    return scraper.get_meta_data()


def download_files_by_type(urls: List[str], file_extensions: List[str]) -> List[str]:
    """
    Download files matching the given file extensions from the specified URLs.

    Args:
        urls (List[str]): List of target URLs.
        file_extensions (List[str]): File extensions to download (e.g., ['.pdf', '.csv']).

    Returns:
        List[str]: List of local file paths for downloaded files.
    """
    scraper = WebScraper(
        urls=urls,
        output_dir=DEFAULT_OUTPUT_DIR,
        file_types=file_extensions,
        search_terms=SEARCH_TERMS,
        max_depth=5
    )

    links = scraper.fetch_links_from_url()
    filtered = scraper.filter_links_by_file_type(links)
    return scraper.download_files(filtered)


def get_structured_snapshots(urls: List[str]) -> List[Dict[str, Any]]:
    """
    Retrieve a structured snapshot (title, headings, links, etc.) from each web page.

    Args:
        urls (List[str]): List of target URLs.

    Returns:
        List[Dict[str, Any]]: Snapshot data per URL as dictionaries.
    """
    scraper = WebScraper(
        urls=urls,
        output_dir=DEFAULT_OUTPUT_DIR,
        file_types=FILE_EXTENSIONS,
        search_terms=SEARCH_TERMS,
        max_depth=5
    )

    snapshots = scraper.get_structured_page_snapshot()
    return [snap.to_dict() for snap in snapshots]


def search_keywords_in_page(urls: List[str], keywords: List[str]) -> Dict[str, List[str]]:
    """
    Search for specific keywords on each page.

    Args:
        urls (List[str]): List of URLs to scan.
        keywords (List[str]): Keywords to search for.

    Returns:
        Dict[str, List[str]]: Dictionary mapping URLs to matched keywords.
    """
    scraper = WebScraper(
        urls=urls,
        output_dir=DEFAULT_OUTPUT_DIR,
        file_types=FILE_EXTENSIONS,
        search_terms=keywords,
        max_depth=5
    )

    return scraper.search_text_for_keywords()


def extract_tables_from_page(urls: List[str]) -> Dict[str, List[List[List[str]]]]:
    """
    Extract HTML tables from each URL.

    Args:
        urls (List[str]): List of URLs containing tables.

    Returns:
        Dict[str, List[List[List[str]]]]: Tables per page (page -> list of tables -> rows -> cells).
    """
    scraper = WebScraper(
        urls=urls,
        output_dir=DEFAULT_OUTPUT_DIR,
        file_types=FILE_EXTENSIONS,
        search_terms=SEARCH_TERMS,
        max_depth=5
    )

    return scraper.extract_tables()


SCRAPING_TOOLS = [
    get_page_metadata,
    get_structured_snapshots,
    download_files_by_type,
    search_keywords_in_page,
    extract_tables_from_page
]

# tool_executor.py


from typing import Callable, Any, List, Dict 
import inspect 
import logging 


logger = logging.getLogger(__name__)

class ToolExecutor:
    """

    Executes a collection of callable tools by name, handles dynamic arguent matching, and supports logging or augmentation for tracing execution.

    """

    def __init__(self, tools: List[Callable[..., Any]]):
        """
        Initialize the executor witha list of tool functions.

        Args:
            tools (List[Callable[...,Any]])


        """
        self.tools = {fn.__name__: fn for fn in tools}


    def has_tool(self, name: str) -> bool:
        """Check if a tool  with the given name is registered"""
        return name in self.tools

    def list_tools(self) -> List[str]:
        """Return a list of all available tool names"""

    def get_signature(self,name: str)-> Dict[str,inspect.Parameter]:
        """
        Return the signature of a registered tool. 

        Args:
            name(str): Name of the tool. 

        Returns:
            Dict[str, inspect.Parameter]: Parameter info for introspection or validation
        """

        if name not in self.tools:
            raise ValueError(f"Tool '{name}' is not registered")
        return inspect.signature(self.tools[name]).parameters 

    def execute(self, name: str, args: Dict[str, Any]) -> Any:
        """
        Call a tool by name with provided arguemnts 

        Args: 
            name(str): Name of the registered tool function. 
            args(Dict[str,Any]): Keyword arguments for the tool. 

        Returns: 
            Any: Result of the tool execution.
        """

        if name not in self.tools:
            raise ValueError(f"Tool '{name}' is not available")

        fn = self.tools[name]
        sig = inspect.signature(fn)

        accepted_args =  {
                k: v for k, v in args.items() if k in sig.parameters
        }

        try:
            logger.info(f"Executing tool: {name} with args: {accepted_args}")
            return fn(**accepted_args)

        except Exception as e:
            logger.exception(f"Error during execution of tool '{name}': {e}")
            raise

#test_hello.py
from python_llm_toolkit.hello import add


def test_add():
    assert add(2,3) ==5 

# test_scraper.python_llm_toolkit

from typing import List, Dict
from python_llm_toolkit.scraper import  WebScraper, SEARCH_TERMS, FILE_EXTENSIONS, PageSnapshot
import requests
import os 
import pytest
import json

home_dir = os.getcwd()

@pytest.fixture
def webscraper():
    return WebScraper(
search_terms = SEARCH_TERMS,
file_types=FILE_EXTENSIONS,
output_dir= home_dir+"/downloads",
max_depth= 5,
urls=["https://smithtech.io","https://google.com","https://www.uchicagomedicine.org/"]
)

def test_WebSraper_Creation(webscraper):
            
    assert isinstance(webscraper,WebScraper)


def test_fetch_links_from_url(webscraper):
    links  = webscraper.fetch_links_from_url()
    assert isinstance(links,list)
    assert links is not None 



def test_get_images_urls(webscraper):
    images = webscraper.get_image_urls()
    assert isinstance(images,list)

def test_get_meta_data(webscraper):
    meta = webscraper.get_meta_data()
    assert all("url" in item for item in meta)


import tempfile 

@pytest.fixture
def test_scraper():
    with tempfile.TemporaryDirectory() as tmp_dir:
        yield WebScraper(
            search_terms=[],
            file_types=['.txt'],
            max_depth=1,
            output_dir=tmp_dir,
            urls=["https://www.w3.org/TR/PNG/iso_8859-1.txt"]  # known test-safe file
        )

def test_download_files_create_files(test_scraper):
    links = test_scraper.fetch_links_from_url()
    links.append("https://www.w3.org/TR/PNG/iso_8859-1.txt")

    filtered = test_scraper.filter_links_by_file_type(links) 

    downloaded = test_scraper.download_files(filtered)
    
    assert isinstance(downloaded,list)
    assert len(downloaded) > 0 
    for path in downloaded:
        assert os.path.exists(path)
        assert os.path.isfile(path)

def test_downloaded_files_handles_bad_link(test_scraper):
    bad_links=  ["https://example.com/thisfiledoesnotexist.pdf"]

    result = test_scraper.download_files(bad_links)
    assert result == []
    assert len(test_scraper.files_downloaded) == 0 


def test_download_files_creates_subfolders(test_scraper):
    links = ["https://www.w3.org/TR/PNG/iso_8859-1.txt"]
    downloaded = test_scraper.download_files(links)

    assert len(downloaded) == 1
    file_path = downloaded[0]
    # subfolder path should contain the domain and file extension
    assert "w3.org" in file_path
    assert "txt" in file_path

def test_get_headings(test_scraper):
    headings = test_scraper.get_heading()

    print(headings)

    # Check it returns a dicitonary vwith correct keys 
    assert set(headings.keys()) == {f"h{i}" for i in range(1,7)}
    
    # Check thate each value is a list 
    assert all(isinstance(v,list) for v in headings.values())

    # Check if any heading was found at all
    found_any = any(len(v) > 0 for v in headings.values())
    assert isinstance(found_any,bool)

def test_search_text_for_keywords(webscraper):
    
    result  = webscraper.search_text_for_keywords()
    print(result)
    assert isinstance(result,dict)

    for url, keywords in result.items():
        print(f"{keywords} were found in {url} ")
        assert isinstance(url,str)
        assert isinstance(keywords,list)
        assert all(isinstance(k,str) for k in keywords)

def test_extract_links_with_text(webscraper):

    extracted = webscraper.extract_links_with_text()
    
    assert isinstance(extracted, dict)

    for url, text in extracted.items():
        if text:
            assert "href" in text[0]
            assert "text" in text[1]

        assert isinstance(url,str)
        assert isinstance(text,list)
        assert all(isinstance(t,dict) for t in text)
        
def test_extract_tables(webscraper):
    tables = webscraper.extract_tables()

    assert isinstance(tables,dict) 


def test_extract_main_text(webscraper):
    main_text = webscraper.extract_main_text()

    assert isinstance(main_text,dict)

    for url, text in main_text.items():
        assert isinstance(url,str)
        assert isinstance(text,str)
        assert len(text) >=0 


def test_extract_json_ld(webscraper):
    data = webscraper.extract_json_ld()

    assert isinstance(data,dict)

    for url,items in data.items():
        assert isinstance(url,str)
        assert isinstance(items,list)
        for item in items:
            assert isinstance(item, dict)


def test_get_structured_page_snapshot(webscraper):
    snapshots = webscraper.get_structured_page_snapshot()

    assert isinstance(snapshots,list)

    assert len(snapshots) >= 0 

    for snap in snapshots:
        assert isinstance(snap,PageSnapshot)
        assert isinstance(snap.url,str)
        assert snap.url.strip() != ""
        assert isinstance(snap.title,str)


def test_export_snapshots_to_json(tmp_path, webscraper):
    snapshots = webscraper.get_structured_page_snapshot()


    output_file = tmp_path / "snapshots.json"
    webscraper.export_snapshots_to_json(output_file, snapshots)

    assert output_file.exists()


    with open(output_file,"r") as f:
        data = json.load(f)

        assert isinstance(data,list)
        assert all("url" in item for item in data)




def test_load_snapshots_from_json(tmp_path,webscraper):

    snapshots = webscraper.get_structured_page_snapshot()


    json_path = tmp_path/ "snapshots.json"
    webscraper.export_snapshots_to_json(str(json_path),snapshots)


    loaded = webscraper.load_snapshots_from_json(str(json_path))

    assert isinstance(loaded,list)

    assert len(loaded) == len(snapshots)

    for snap in loaded:
        assert isinstance(snap,PageSnapshot)
        assert isinstance(snap.url, str)
        assert isinstance(snap.headings,dict)

def test_export_and_load_csv(tmp_path, test_scraper):
    snapshots = test_scraper.get_structured_page_snapshot()
    
    csv_path = tmp_path / "snapshots.csv"
    
    # Export
    test_scraper.export_snapshots_to_csv(str(csv_path), snapshots)
    assert csv_path.exists()

    # Load
    loaded = test_scraper.load_snapshots_from_csv(str(csv_path))
    assert isinstance(loaded, list)
    assert all(isinstance(row, dict) for row in loaded)
    assert all("url" in row and "title" in row for row in loaded)

